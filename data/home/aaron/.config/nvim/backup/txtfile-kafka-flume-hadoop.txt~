# Objective: Move flat file to kafka and use flume to move the data to HDFS

# Step 1 : Start Hadoop
# Start Hadoop :  start-all.sh
# or
# start-dfs.sh followed by start-yarn.sh

# access HDFS: http://localhost:9870 & http://localhost:8088

# create a new folder for logs in hdfs
# hdfs dfs -mkdir logs
# hdfs dfs -ls / # root dir
# hdfs dfs -ls hdfs://localhost:9000

# Step 2 : Start Kafka, so as to load the log file
# Start Zookeeper
  bin/zookeeper-server-start.sh config/zookeeper.properties

# Start Kafka
  bin/kafka-server-start.sh config/server.properties

# Create Topic (Simple)
bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic logs

###### Mostly FYI ##########################
# This section is for inserting data from mysql to kafka
# list topic
# bin/kafka-topics.sh --list --bootstrap-server localhost:9092

# Start Producer
# bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test

# Start Consumer
# bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
#

# create a Kafka topic for  a table named customers in the database demo in mysql
# do not forget to install & call debezium mysql connector and update the "standalone" connector file

# kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic asgard.demo.customers
##### END FYI #####

# Start connector ####################
# connect-standalone.sh /usr/local/kafka/config/connect-standalone.properties  /usr/local/kafka/config/connect-file-source.properties
# Connector properties
# Connect-Standalone.properties #######
## bootstrap.servers=localhost:9092
## key.converter=org.apache.kafka.connect.json.JsonConverter
## value.converter=org.apache.kafka.connect.json.JsonConverter
## key.converter.schemas.enable=false
## value.converter.schemas.enable=false
## offset.storage.file.filename=/tmp/connect.offsets
## offset.flush.interval.ms=10000
## plugin.path=/usr/local/kafka/connect/
#######################################
# connect-file-source.properties
##
## name=local-file-source
## connector.class=FileStreamSource
## tasks.max=1
## file=/home/aaron/projects/hadoop/3_error.log.txt
## topic=logs
######################################

# Step 3 : Start Flume, so as to move the log from Kafka to HDFS
# Â flume-ng agent --conf conf -conf-file conf/flume-kafka-source-hdfs-sink.conf --name agent1
# Agent Properties
## flume-kafka-source-hdfs-sink.conf #
## agent1.sources = kafka1
## agent1.channels = channel1
## agent1.sinks = hdfs1

## agent1.sources.kafka1.type = org.apache.flume.source.kafka.KafkaSource
## agent1.sources.kafka1.zookeeperConnect = localhost:2181
## agent1.sources.kafka1.kafka.bootstrap.servers = localhost:9092
## agent1.sources.kafka1.kafka.topics = logs
## agent1.sources.kafka1.batchSize = 1000
## agent1.sources.kafka1.groupId = flume
## agent1.sources.kafka1.channels = channel1
## agent1.sources.kafka1.interceptors = i1
## agent1.sources.kafka1.interceptors.i1.type = timestamp
## agent1.sources.kafka1.kafka.consumer.timeout.ms = 100

## agent1.channels.channel1.type = memory
## agent1.channels.channel1.capacity = 10000
## agent1.channels.channel1.transactionCapacity = 10000
## agent1.channels.channel1.byteCapacityBufferPercentage = 20
## agent1.channels.channel1.byteCapacity = 6912212

## agent1.sinks.hdfs1.type = hdfs
## agent1.sinks.hdfs1.hdfs.path = hdfs://localhost:9000/user/aaron/logs
## agent1.sinks.hdfs1.hdfs.rollInterval = 5
## agent1.sinks.hdfs1.hdfs.rollSize = 0
## agent1.sinks.hdfs1.hdfs.rollCount = 10000
## agent1.sinks.hdfs1.hdfs.fileType = DataStream
## agent1.sinks.hdfs1.channel = channel1
############################################

# Verify if it worked
# hadoop fs -cat hdfs://localhost:9000/user/aaron/logs/* | grep -o 'error' | wc -l

