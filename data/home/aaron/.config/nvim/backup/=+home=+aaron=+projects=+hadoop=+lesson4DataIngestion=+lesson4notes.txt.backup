Data Lake: large repository that stores large amounts of structured, semi-structured and unstructured data
Big data ingestion tools:

|---------------|
| Apache Sqoop: |
|---------------|
Command line interface application to transfer data from RDBMS to Hadoop , Hive or Hbase. It can also export data from Hadoop to RDBMS. Sqoop works with oozie to schedule automating import and export tasks. The data being imported is subdivided into partitions. Next a map only job is launched with individual mappers responsible for transferring slices of the dataset. Lastly each record in the dataset is handled in a type safe manner.

Importing data using squoop
squoop import --connect jdbc:mysql://localhost/simplilearn  --username training --password pswd --table customers

SQOOP processing
SQOOP introspects the database to gather the necessary metadata for the data being imported. A map-only Hadoop job is submitted to the cluster via SQOOP. Then the map-only job performs data transfer using the metadata captured.

SQOOP Import Process
Imported data is saved in a directory on HDFS based on the table being imported.

SQOOP Export
SQOOP export --connect jdbc:mysql://localhost/simplilearn --username training --password psswd  --export-dir /data/exportoutput/part-00000 --table customers --input-fields-terminated-by '\t'

SQOOP Connectors
Generic JDBC connector : used to connect to any database that is accessible through JDBC
Default SQOOP connector : Designed for specific databases such as MySQL, PostresSQL, ORACLE, SQL Server & DB2
First-Path connector : Specializes in using specific batch tools to transfer data with high throughput

Controlling parallelism
by default SQOOP imports data using four parallel tasks called mappers. Increasing the number of tasks may increase the import speed. But, each task will add a load to the DB server. Number of tasks can be influenced using the -m or --num-mappers attribute.

Sample SQOOP commands
SQOOP import -driver com.mysql.jdbc.driver --connect jdbc:mysql://localhost/simplilearn -username root --password root -table squoo_demo -target-dir /usr/sqoop_batch5 -m 1  -as-textfile

SQOOP import -driver com.mysql.jdbc.driver --connect jdbc:mysql://localhost/simplilearn -username root --password root -table squoo_demo -target-dir /usr/sqoop_batch5 -m 1  -as-textfile  --where "id>2"

SQOOP import --driver com.mysql.jdbc.driver --connect jdbc:mysql://localhost/simplilearn --username root --password root --table squoo_demo --target-dir /usr/sqoop_batch3 --e "Select * from sqoop_demo where id=13" -m 1 as-textfile

// show databases
sqoop list-databases  --connect jdbc:mysql://localhost/test --username root --password hello6559

//show tables
sqoop list-tables  --connect jdbc:mysql://localhost/test --username root --password hello6559

// Show table rows
Â sqoop eval  --connect jdbc:mysql://localhost/test --username root --password hello6559 -e "Select * from employee"


Limitations of SQOOP: JDBC drivers must be installed, Client side architecture needs connectivity to the cluster, user has to specify username and password, difficult to integrate a cli within external applications, does not support nosql DB because it is tightly coupled with JDBC semantics

|---------------|
| Apache Flume: |
|---------------|

Distributed and reliable service for efficiently collecting, aggregating and moving large amounts of streaming data into Hadoop Distributed File System (HDFS). Robust and fault tolerant.
Flume consists of Source, Sink and Channel. The source consumes events delivered to it by an external source such as a web server. The source stores the events in one or more channels. Channel is a passive store that keeps the events until it is consumed by the flume sink. Channels buffer events from the source until they are drained by the sink and act as a bridge between the source and the sink

Flume Model
Flume model consists of Agent processor and collector. Agents assure the continuous ingestion of data. Processor performs intermediate processing of the jobs. The collector component  is responsible for writing data to the permanent HDFS storage.

Flume goals
Tunable failure recovery modes
Extensibility through plug-in architecture
Horizontally scalable data paths  to form a topology of agents
Centralized data flow management interface

Common Flume data sources: log files, unix syslog, program output, Sensor data, social media posts, network sockets, status updates

Netcat: listens on  given port and turns the text into an event
Kafka: receives events as a messages from a KAFKA topic
Syslog: captures messages from the unix syslog daemon
Spooldir: shared area on a disk

Fume Sink
Null: Discards all events received
HDFS
HBASE

Flume Channels
Memory
File
JDBC : Database table

Flume Configuration File
|-------------------------------+---------------------|
| Attribute                     | Value               |
|-------------------------------+---------------------|
| agent1.sources=               | src1                |
| agent1.channels=              | ch1                 |
| agent1.sinks=                 | sink1               |
|-------------------------------+---------------------|
| Agent1.channels.ch1.type=     | memory              |
|-------------------------------+---------------------|
| Agent1.sources.src1.type=     | spooldir            |
| agent1.sources.src1.spooldir= | /var/flume/incoming |
| agent1.sources.src1.channels= | ch1                 |
|-------------------------------+---------------------|
| Agent1.sinks.sink1.type=      | hdfs                |
| Agent1.sinks.sink1.hdfs.path= | /data/dir/          |
| Agent1.sinks.sink1.channel=   | ch1                 |
|-------------------------------+---------------------|

Flume-ng agent -n twitteragent -c conf -f flume.conf

|--------------|
| Apache Kafka |
|--------------|
High performance real time messaging system.

Characteristics are
Distributed and partitioned messaging system
Highly fault tolerant and scalable
Process and send millions of messages / sec to several receivers

Use cases
Messaging services: Send and receive millions of msgs  in real time
Real time stream processing: Process continuous stream of information in real time and pass to stream processors such as storm
Log aggregation: collect physical log files from multiple systems $ store them in HDFS
Commit log service: external commit log for distributed systems
Event sourcing: maintain time ordered  sequence of events
Website activity tracking: process real time web site activities like page views, searches etc

Kafka Data model
Kafka data model consists of messages and topics
messages represent information such as lines in a log file.
Messages are grouped into categories called topics example log message, stock message

The process that publishes message into a topic in Kafka are known as producers
Processes that receive a topic in Kafka are called consumers.
Processes are servers within Kafka that process the messages are known as brokers
A Kafka cluster consists of a set of brokers

Topics are a category of messages  in Kafka.  Producers publish the messages into different topics. Consumers then read the messages from the topic that they are interested in. Topics can be divided into partitions for parallelism.  Kafka-topics.sh command to create and modify topics
Example
create-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 3 --topic test
the replication factor is for fault tolerance

Partitions: Topics can be divided into partitions which are the unit of parallelism or horizontal expansion within Kafka. Partitions allow messages in a topic to be distributed to multiple different servers. A topic can have any number of partitions. Each partition should fit into a single Kafka server. The number of partitions @decide the level of parallelism  of the topic.

Partition Distribution: Partitions can be distributed across the Kafka cluster. Each Kafka cluster may handle one or more partitions. Partitions can be distributed across several Kafka servers. One partition may be elected as the leader and the others are followers. The leader controls the read and write for partitions whereas the followers replicate the data. If the leader fails, one of the followers will automatically becomes the leader. Zookeeper is used for the leader selection process.

Producers: producers place the messages in a topic. They also decide which partition to place the topic. Topics should already exist before the messages can be placed. Messages are placed using a FIFO algorithm.

Consumers are the receivers of the messages in Kafka.  Consumer belongs to a consumer group. A consumer group may have one or more consumers. Consumers specify what topics that they want to listen to. A message is send to all the consumers in a consumer group.

Kafka Architecture

Kafka broker consist of multiple brokers (in a cluster) in order to support load balancing . Brokers are stateless and are managed by the Zookeeper for state as well as leader election.

 Zookeeper service is used for managing and coordinating the Kafka brokers. It also notifies the producers and consumers if a new broker enters or the failure of a broker in  the system.
 The consumer have to maintain the message state (since brokers are stateless) 

 Kafka APIS are available for producers, consumers, connect and stream processors

 The producer API allows applications to send stream of data to topics in the Kafka cluster
 Streams API allows transforming streams of data from input to output topics.
 Consumer API allows applications to read streams of data from topics in the Kafka cluster
 Connect  API allows data ingestion from source systems into Kafka  or push from Kafka into some sink systems.

Producer API
Set up producer configuration 
Get a handle of the producer connection
Create message as a Key-value pair
Submit the message to a particular topic
close the connection

Consumer API
Setup consumer configuration
Get a handle on the consumer connection
Get a stream of messages for the topic
Loop over the messages and process them
close the connection

Kafka Connect: Framework for connecting Kafka with external systems such as databases, key-value stores, search indexes and file systems using connectors
Popular connectors: Kafka connect JDBC, Kafka Connect S3 (Sink in AVRO or JSON format), SAP Hana Connector.

Demo
Start Zookeeper: Zookeper-server-start.sh config/zookeper.properties
Start Kafka: Kafka-server-start.sh config/server.properties
Create Kafka Topic: kafka-topics.sh --create --zookeeper localhost:2101 --replication-factor 1 --partitions 1 --topic mytest
Start Producer: Kafka-console-producer.sh --broker-list localhost 9092 --topic mytest
Enter messages
Start Kafka Consumer: 

 
 
