Technology Orientation Manual
ASP.Net Performance
	Common Gotchas
		1. Enable compression: The HTTP protocol is not a particularly efficient protocol and, by default, there is no compression of the content. Some web resources are already compressed, especially images, but HTML, CSS and JavaScript are typically transferred as text. Even the most archaic of browsers support compression of HTTP content using the gzip algorithm.
		2. Reduce HTTP requests: Every time the browser needs to open a connection to the server there a tax that must be paid. This tax is in the form of TCP/IP connection overhead. This problem is especially noticeable in scenarios with high latency where it takes a long time to establish these new connections. Add to this the fact that browsers limit the number of requests they will make to a single server at once, and it becomes apparent that reducing the number of HTTP requests is a great optimization.
		3. Depending on the type of the resource being requested from the server, there are a few different approaches to reducing the number of requests. For JavaScript, concatenating the scripts together into a single file using a tool like webpack, gulp or grunt can bundle together all the JavaScript into a single file. Equally, we can combine CSS files into a single file using tasks in the same build tools we use for JavaScript. If the site uses a number of small images, then a technique called CSS Spriting can be used. In this, where we combine all the images into a single one and then use CSS offsets to shift the image around and show just the single sprite we want.
		4. HTTP/2 over SSL: The new version of HTTP, HTTP/2, introduces a number of very useful optimizations. First, the compression we spoke of in #1 has been extended to also cover the protocol headers. More interestingly, the connection with the server can transfer more than one file at a go using a mechanism known as "pipe lining". This means that the reduction of HTTP requests by combining files is largely unnecessary.
		5. Minify your files: Compression is a great tool for reducing the amount of data sent over the wire, but all the compression algorithms used to send HTML, CSS and JavaScript are lossless compression algorithms. This means that the result of doing compress(x) => decompress(x) always equals x. With some understanding of what it is that is being compressed, we can eek out some additional gains in size reduction. For instance, the JavaScript
			function doSomething(){
				var size_of_something_to_do = 55;
				for (var counter_of_stuff = 0;
					counter_of_stuff < size_of_something_to_do;
					counter_of_stuff++) {
					size_of_something_to_do--;
				}
			}
		is functionally equivalent to
			function doSomething(){var a=55;for(var b=0;b<a;b++){a--;}}

		This is because the scope of the variables is entirely private and the whitespace largely unnecessary. This process is called minification. Similar compression techniques can be applied to CSS and even to HTML.
		6. Load CSS first: Load the CSS content of your site first, preferably in the head section of the page. To understand the reasoning here, you need to understand a little bit about how browsers achieve their incredible speed. When downloading a page, the browser will attempt to start rendering the application as soon as it has any content. Often what it renders is something of a guessing game because the browser doesn't know what content on the page may invalidate the guesses it has made. When the browser realizes that it has made an incorrect guess about how the page should be rendered, then all the work that was done needs to be thrown out and started over again. One of the things which causes one of these reflows is the addition of a new stylesheet. Load style sheets first to avoid having a style that alters an already-rendered element.
		7. Load JavaScript last: JavaScript is a complete about-face from CSS, and should be loaded last. This is because we want the page to render as quickly as possible, and JavaScript is not typically necessary for the initial render. Users will typically take a moment to read the page and decide what to do next. This window is be used to load scripts in the background and light up the page's interaction. There is a caveat to this rule: if your site is a heavy user of JavaScript, for instance, such as an Angular or React application, then you may find that loading JavaScript last is actually detrimental. You may wish to investigate loading only the JavaScript necessary to bootstrap the application, and loading more in the background. If speed is really important, you can even investigate what are called isomorphic or universal applications. The pages in these applications are rendered on the server side, and then the JavaScript application attaches to the already-rendered HTML and takes over from there.
		8. Check Queries: ORMs (object-relational mappers) have been highly beneficial in increasing developer productivity, however they provide a layer of abstraction that can introduce sub-optimal queries. Prefix will highlight times when you might have n + 1 select errors, or are retrieving too much data from the server. It is surprising how easy it is to fix these problems by using eager loading over lazy loading, and examining projections.
		9. Cache parts of your pages: cache only part of your page; this is colloquially known as donut hole caching. It is a useful approach when you have user-specific data mixed with general data on the same page. The user data varies by the user, while the rest of the page is the same for all users. In MVC 5 applications, this is done using partial views, and in MVC Core using caching tag helpers.
		10. Content delivery network (CDN): The speed of light is 300000 km/s, which is pretty jolly quick; but despite this high speed, it actually helps to keep your data close to your consumers. There are a ton of content delivery networks, which have edge nodes very close to wherever your users might be.
		11.  Shrink your libraries: If you’re making use of libraries like jQuery, consider that you may not be using all the functions and can use a smaller, more focused library. Zeptojs is a library which supports many of the features of jQuery, but is smaller. Other libraries like jQuery UI provide for constructing personalized packages with features removed. If you’re on Angular, then the production build performs tree shaking to remove entire chunks of the library that aren't in use for your project. This reduces the payload sent over the wire, while preserving all the same functionality.
		12.  Avoid client-side redirects: The final tip is to avoid redirecting users through the use of client-side redirects. Redirects add an extra server trip that, on high-latency networks like cellular networks, is undesirable. Instead, leverage server-side redirects – these don’t add the extra server trip. One place where this won’t work is redirecting users to an SSL version of your page. For that scenario, HTTP Strict Transport Security and a preload list is your ticket. Adding your site to this preload list will automatically direct traffic to the SSL version of your site.
		13.  Memory leaks involving unmanaged resources: Unmanaged memory is often leaked when you forget to write code to deallocate it; however this can also happen because references are retained to the managed object which is responsible for deallocating it. In other words, unmanaged memory can be held on to by managed memory; so even when it looks like you have an unmanaged problem, it can instead be a problem with your managed code. Code profilers like ANTS Memory Profiler can help demonstrate these kinds of leaks by showing you the chains of references holding unmanaged data in memory. In the example below, the majority of the memory used by the instance of System.Drawing.Bitmap (in black) is unmanaged, so may appear as an unmanaged memory leak, but the Instance Retention Graph in ANTS Memory Profiler demonstrates that it is in fact held in memory by Mandelbrot.Form1, a managed object.
		14.  String concatenation in a loop: string is immutable, which can cause problems when you want to compose a large string from several elements. If you attempt this with simple string concatenation, you'll create a large number of small objects which must be garbage collected, using up lots of memory and putting unnecessary pressure on the GC. If you're constructing a string with a loop, you should use StringBuilderinstead of string concatenation. Let's say you have the list ["a", "b", "c", "d", "e", "f"] and want to join the elements together to create "abcdef". Using string concatenation, a new string will be created on each iteration, whereas StringBuilderwill mutate the existing string. Instead of creating a new string "abc" on the third iteration, for example, it will append "c" to the value "ab". When you're working with large strings, the difference in performance and memory usage between creating a new string and modifying a StringBuildercan be significant.
		15.  Resizing List<T>: A major appeal of Listis the ease with which you can add items. Some typical code to do this might look like:
			private List<string> Tokenize (string data)
			{
				var returnValue = new List<string>();
				string[] tokens = data.Split(new char[] { ':' }).Select(func).ToList();
				foreach (string item in tokens)
				{
					returnValue.Add(func(item));
				}
				return returnValue;
			}
			This can introduce memory problems, however. List<T> is implemented using a simple array. When the backing array is full and you add new items, a new array double the size of the existing array is created. Consequently, if the Listisn't initialized with the required size, it might be resized several times, decreasing the performance and creating lots of intermediate arrays that will have to be garbage collected. The other implication is that a large array may actually use nearly twice as much memory as expected. To avoid this, use the overloaded constructor for the Listwhich allows you to set an initial capacity and so reduce the number of resizing operations.
		16. Unnecessary boxing: Value types are converted to reference types through a process known as boxing, and converted back into value types through unboxing. Boxing a value type creates a new reference type, then copies the value of the value type into the newly created reference type. This takes time and a little bit more than twice the memory of the original value type. Consequently, although boxing is sometimes necessary, it should generally be avoided, because it slows down performance and increases memory requirements.
		Boxing can be avoided by using parameterized classes and methods, implemented using generics. Calls to the BoxedCallmethod will perform a boxing operation on value types and calls to the NonboxedCallmethod will not:

			public void BoxedCall(object value)
			{
			// Perform operation on value
			}

			public void NonboxedCall(T value)
			{
			// Perform operation on value
			}
			Using generics, the type can be determined at compile time, which will improve code performance and prevent an object being boxed onto the heap that will have to be collected later.
		17. Large objects hanging off a delegate closure: Variables created outside of an anonymous method, but used within it, are captured in a closure. This allows the delegate to access the value even when the variable goes out of scope. The compiler does the heavy lifting here, creating a class and assigning values to it every time you use an outside variable. For example, once the compiler has worked on the closure in this example:
			public Person WhoReportsTo (List<Employee> data, Person targetEmployee)
			{
			 return data.Find(d => d.ReportsTo == targetEmployee);
			}
		We end up with:

			[CompilerGenerated]

				private sealed class <>c__DisplayClass4
				{
					 // Fields

					 public Person targetEmployee;

					 // Methods

					 public bool <WhoReportsTo>b__3(Employee d)
					{
					return (d.ReportsTo == this.targetEmployee);
					 }
				}
			The anonymous method is enclosed in a dynamic compiler-generated class, which includes a member variable for every external variable. This class, all the enclosed variables, and anything else that is referenced by it will stay alive as long as the delegate is accessible. This extended lifetime can keep objects in memory for far longer than they otherwise would be. Potentially, it can eat up a sizeable chunk of memory and create situations that look like a memory leak, so avoid closures with memory intensive variables.


	Measuring Performance During Development
		1. PerfTips and Profiler: PerfTips measure the time that passes between two debugger breaks. It is irrelevant whether you use ‘Step Into’, ‘Step Over’ or ‘Run to Cursor’ to measure a single instruction or an entire block of code. The measured timespan is not only displayed in the PerfTip, but also recorded as a list in the Diagnostic Tools window.PerfTips provides quick and continuous feedback on the performance of our application code during development.
		2. Event Tracing for Windows (ETW) and PerfView: PerfView is a self-contained analysis tool for ETW and the CLR. Microsoft uses it internally for many of the teams, and it's the primary performance investigation tool for the .NET Runtime team. PerfView also works for windows docker containers, however it has to be windows 10 1709 or later. On docker containers perfView only gives kernel level events. It has the ability to collect Event Tracing for Windows (ETW) data to trace the call flow of processes identifying the frequency with which functions are called. In addition to profiling process performance data (something tools like Perfmon, PAL and Xperf can't easily do), PerfView also has the ability to analyse process memory heaps to help determine if memory is being used efficiently. It also has a Diff capability that allows you to determine any differences between traces to help spot any regressions. Finally, the tool has a Dump capability to generate a process memory dump.
		3. Micro Benchmarking with BenchmarkDotNet: BenchmarkDotNet is a powerful .NET library for benchmarking. Benchmarking is really hard (especially microbenchmarking), you can easily make a mistake during performance measurements. BenchmarkDotNet will protect you from the common pitfalls (even for experienced developers) because it does all the dirty work for you: it generates an isolated project per each benchmark method, does several launches of this project, run multiple iterations of the method (include warm-up), and so on. Usually, you even shouldn't care about a number of iterations because BenchmarkDotNet chooses it automatically to achieve the requested level of precision. It's really easy to design a performance experiment with BenchmarkDotNet. Just mark your method with the [Benchmark] attribute and the benchmark is ready. Want to run your code on .NET Framework, .NET Core, CoreRT, and Mono? No problem: a few more attributes and the corresponded projects will be generated; the results will be presented at the same summary table. In fact, you can compare any environment that you want: you can check performance difference between processor architectures (x86/x64), JIT versions (LegacyJIT/RyuJIT), different sets of GC flags (like Server/Workstation), and so on.
		4. .NET Core Performance Diagnostic on Linux: Though .NET was originally created to run on Windows, .NET Core makes it possible to run C#, F#, and Visual Basic applications on Windows, Linux, and macOS. Everything required to build and run .NET Core applications is open source (MIT licensed) and available on GitHub. This project is central enough that .NET Core is fully-featured, and supported by Microsoft in production. On windows ETW is used to collect both kernel and a user space data, it offers the ability to collect trace data that spans the entire application (including runtime and libraries) and the Windows kernel. However, nothing like this is available on Linux. The .NET team picked LTTng and perf because each provides enough of the features from ETW to fill the gap and both are widely used by the Linux community. perf is used to collect machine-wide hardware counters (for example, CPU cycles) and kernel events, and LTTng handles user space (runtime services and application-level events) by using trace points generated at CoreCLR build time. Specifically, these tools together provide:
			Hardware counters and kernel events with kernel and user space call stacks.
			Runtime and application-level event-driven tracing data for diagnostic and performance analysis.
		5. Performance Monitoring on the Developer Machine with Stackify Prefix: Stackify Prefif is a profiling tool that allows you to get continuous feedback on the performance characteristics of your server side code, SQL and method execution timings. Prefix attaches itself to IIS or IIS Express and displays information about each web request. Information such as
				The total time it took to serve the page
				The number of database requests that were made
				Exceptions (highlighted in red) that were  caught
				The status code returned by the server
		It allows you to drill down into the request to view things such as
				Raw HTTP request information
				The actual SQL queries being run
				Exception and stack trace information along with the performance impact of that exception
				Timings for each stage of execution
		6. Performance Monitoring on the Developer Machine with MiniProfiler, Glimpse: Miniprofiler tracks database queries and can provide more details if you add some code in your app for the steps that you want to track. It does not use the .NET CLR profiler. You can also change your code to report additional steps within your code to include in the pseudo profile traces. It only works with web apps. Glimpse does not use the .NET CLR profiler. Utilizes an extension and packages framework to add support for various app dependencies and technologies.

	General tips for writing performant code
		1. Value Types vs. Reference Types and Reducing Pressure on the GC:
			Every instance of a reference type has two extra fields used internally by CLR.
			Value Types have no hidden overhead, so they have better data locality.
			Reference Types are managed by GC. It tracks the references, offers fast allocation and expensive, non-deterministic deallocation.
			Value Types are not managed by the GC. Value Types = No GC. And No GC is better than any GC!
			Whenever a reference is required value types are being boxed. Boxing is expensive, adds an extra pressure for the GC. You should avoid boxing if you can.
			By using generic constraints we can avoid boxing and even de-virtualize interface method calls for Value Types!
			Value Types are passed to and returned from methods by Value. So by default, they are copied all the time.
			Avoid copying big Value Types? We should pass and return them by Reference!
		2. Saving Threads with async/await: You can avoid performance bottlenecks and enhance the overall responsiveness of your application by using asynchronous programming. However, traditional techniques for writing asynchronous applications can be complicated, making them difficult to write, debug, and maintain. C# 5 introduced a simplified approach, async programming, that leverages asynchronous support in the .NET Framework 4.5 and higher, .NET Core, and the Windows Runtime. The compiler does the difficult work that the developer used to do, and your application retains a logical structure that resembles synchronous code. As a result, you get all the advantages of asynchronous programming with a fraction of the effort. Asynchrony is essential for activities that are potentially blocking, such as web access. Access to a web resource sometimes is slow or delayed. If such an activity is blocked in a synchronous process, the entire application must wait. In an asynchronous process, the application can continue with other work that doesn't depend on the web resource until the potentially blocking task finishes. When you use asynchronous methods, the application continues to respond to the UI. You can resize or minimize a window, for example, or you can close the application if you don't want to wait for it to finish.
		3. Choosing the Right Collection: The .NET BCL has lots of collections built in to help you store and manipulate collections of data. Understanding how these collections work and knowing in which situations each container is best is one of the key skills necessary to build more performant code. Choosing the wrong collection for the job can make your code much slower or even harder to maintain if you choose one that doesn't perform as well or otherwise doesn't exactly fit the situation. Remember to avoid the original collections and stick with the generic collections.  If you need concurrent access, you can use the generic collections if the data is read-only, or consider the concurrent collections for mixed-access if you are running on .NET 4.0 or higher. Consider immutable collections if their behaviour and performance match your requirements best.
	C# 7 Performance Tips
		1. Avoid Heap Allocations with Local Functions: Local functions enable you to define a function within the scope of another method to help in promoting encapsulation and bring local variables into scope. The support for local functions is an excellent new feature that has been introduced in C# 7. You can define local functions inside any method, the constructor of a class or inside a property -- both getter and setter. When it's compiled by the C# compiler, a local function gets transformed into a private method. When creating a lambda, a delegate has to be created, which is an unnecessary allocation in this case. Local functions are really just functions, no delegates are necessary.Also, local functions are more efficient with capturing local variables: lambdas usually capture variables into a class, while local functions can use a struct (passed using ref), which again avoids an allocation.
		2. Make ValueTypes Faster with ref return
		3. Optimization of async-methods with ValueTask:
				If an async method completes synchronously the performance overhead is fairly small.
				If the async method completes synchronously the following memory overhead will occur:
					for async Task methods there is no overhead,
					for async Task<T>methods the overhead is 88 bytes per operation (on x64 platform).
				ValueTask<T> can remove the overhead mentioned above for async methods that complete synchronously.
				A ValueTask<T>-based async method is a bit faster than a Task<T>-based method if the method completes synchronously and a bit slower otherwise.
				A performance overhead of async methods that await non-completed task is way more substantial (~300 bytes per operation on x64 platform).
			But you may also try to make your async operations coarser grained. This can improve performance, simplify debugging and overall make your code easier to reason. Not every small piece of code has to be asynchronous.
		4. Pattern matching and Performance: C# 7 introduced the following patterns: the const pattern, the type pattern, the var pattern and the discard pattern. Patterns can be used in is-expressions and in case blocks. The implementation of the const pattern in is-expression for value types is far from perfect from the performance point of view. The var-pattern always match and you should be careful with them. A switch statement can be used for a set of type checks with additional predicates in when clauses.
		5. Performance Characteristics of the C# 7 Tuple Feature: C# 7.0 introduced ValueTuple structure, which is a value type representation of the tuple object. The language team made many good things for this value tuple type, including a new syntax and many features (such as deconstruction.). ValueTuple makes the C# language more modern, and easy to use with simplified syntax. It solves many Tuple problems:
				Value Tuple objects have first class syntax support, it simplifies the code to work with tuple elements.
				You can associate a name with the value tuple element, you get some level of design time and compiler time validation of your code.
				You are now flexible to access all tuple elements, or some of them, by using the deconstruction and the _ keyword.
				Value Tuple types are value types, no inheritance or other features, this means that the value tuples are better performance.
		Since the name of the value tuple element is not runtime, you have to be careful using it when doing serialization with existing libraries, such as Newtonsoft.Json
		6. Avoid blocking calls: ASP.NET Core apps should be designed to process many requests simultaneously. Asynchronous APIs allow a small pool of threads to handle thousands of concurrent requests by not waiting on blocking calls. Rather than waiting on a long-running synchronous task to complete, the thread can work on another request.A common performance problem in ASP.NET Core apps is blocking calls that could be asynchronous. Many synchronous blocking calls leads to Thread Pool starvation and degrading response times.
			Do not:
				Block asynchronous execution by calling Task.Wait or Task.Result.
				Acquire locks in common code paths. ASP.NET Core apps are most performant when architected to run code in parallel.
			Do:
				Make hot code paths asynchronous.
				Call data access and long-running operations APIs asynchronously.
				Make controller/Razor Page actions asynchronous. The entire call stack needs to be asynchronous in order to benefit from async/await patterns.
		7. Minimize large object allocations: The .NET Core garbage collector manages allocation and release of memory automatically in ASP.NET Core apps. Automatic garbage collection generally means that developers don't need to worry about how or when memory is freed. However, cleaning up unreferenced objects takes CPU time, so developers should minimize allocating objects in hot code paths. Garbage collection is especially expensive on large objects (> 85 K bytes). Large objects are stored on the large object heap and require a full (generation 2) garbage collection to clean up. Unlike generation 0 and generation 1 collections, a generation 2 collection requires app execution to be temporarily suspended. Frequent allocation and de-allocation of large objects can cause inconsistent performance.

			Recommendations:

				Do consider caching large objects that are frequently used. Caching large objects prevents expensive allocations.
				Do pool buffers by using an ArrayPool<T> to store large arrays.
				Do not allocate many, short-lived large objects on hot code paths.
			Memory issues like the preceding can be diagnosed by reviewing garbage collection (GC) stats in PerfView and examining:

				Garbage collection pause time.
				What percentage of the processor time is spent in garbage collection.
				How many garbage collections are generation 0, 1, and 2.
		8. Optimize Data Access: Interactions with a data store or other remote services are often the slowest part of an ASP.NET Core app. Reading and writing data efficiently is critical for good performance.

			Recommendations:

				Do call all data access APIs asynchronously.
				Do not retrieve more data than is necessary. Write queries to return just the data that is necessary for the current HTTP request.
				Do consider caching frequently accessed data retrieved from a database or remote service if it is acceptable for the data to be slightly out-of-date. Depending on the scenario, you might use a MemoryCache or a DistributedCache.
		9. Pool HTTP connections with HttpClientFactory: Although HttpClient implements the IDisposable interface, it's meant to be reused. Closed HttpClient instances leave sockets open in the TIME_WAIT state for a short period of time. Consequently, if a code path that creates and disposes of HttpClient objects is frequently used, the app may exhaust available sockets. HttpClientFactory was introduced in ASP.NET Core 2.1 as a solution to this problem. It handles pooling HTTP connections to optimize performance and reliability.

			Recommendations:

				Do not create and dispose of HttpClient instances directly.
				Do use HttpClientFactory to retrieve HttpClient instances. For more information, see Use HttpClientFactory to implement resilient HTTP requests.
		10. Keep common code paths fast: You want all of your code to be fast, but frequently called code paths are the most critical to optimize:

				Middleware components in the app's request processing pipeline, especially middleware run early in the pipeline. These components have a large impact on performance.
				Code that is executed for every request or multiple times per request. For example, custom logging, authorization handlers, or initialization of transient services.
			Recommendations:

				Do not use custom middleware components with long-running tasks.
				Do use performance profiling tools (like Visual Studio Diagnostic Tools or PerfView) to identify hot code paths.
		11. Complete long-running Tasks outside of HTTP requests: Most requests to an ASP.NET Core app can be handled by a controller or page model calling necessary services and returning an HTTP response. For some requests that involve long-running tasks, it's better to make the entire request-response process asynchronous.

			Recommendations:

				Do not wait for long-running tasks to complete as part of ordinary HTTP request processing.
				Do consider handling long-running requests with background services or out of process with an Azure Function. Completing work out-of-process is especially beneficial for CPU-intensive tasks.
				Do use real-time communication options like SignalR to communicate with clients asynchronously.
		12. Minimize exceptions: Exceptions should be rare. Throwing and catching exceptions is slow relative to other code flow patterns. Because of this, exceptions should not be used to control normal program flow.

			Recommendations:

				Do not use throwing or catching exceptions as a means of normal program flow, especially in hot code paths.
				Do include logic in the app to detect and handle conditions that would cause an exception.
				Do throw or catch exceptions for unusual or unexpected conditions.
			App diagnostic tools (like Application Insights) can help to identify common exceptions in an application which may affect performance.

	Performance Related API's
		1. Reusing Arrays with ArrayPool<T>: Provides a resource pool that enables reusing instances of type T[]. Using the ArrayPool<T> class to rent and return buffers (using the Rent and Return methods) can improve performance in situations where arrays are created and destroyed frequently, resulting in significant memory pressure on the garbage collector.
		2. Accessing all Types of Memory Safely and Efficiently with Span<T>: Span<T> and Memory<T> are new features in .NET Core 2.1 that allow strongly-typed management of contiguous memory, independently of how it was allocated. These allow easier to maintain code and greatly improves the performance of applications by reducing the number of required memory allocations and copies. Span<T> can only exist in the stack (as opposed to existing in the heap). This means it can’t be a field in a class or in any “box-able” struct (convertible to a reference type). Span<T> takes advantage of ref struct, a new feature in C# 7.2, making the compiler enforce this rule. Span<T> and Memory<T> are new features that can drastically reduce the memory copies in .NET applications, allowing performance improvements without sacrificing type safety and code readability.
	Data Access Performance: Entity Framework Core
		1. Being too greedy with Rows: At its heart, Entity Framework is a way of exposing .NET objects without actually knowing their values, but then fetching / updating those values from the database behind the scenes when you need them. It’s important to be aware of when EF is going to hit the database – a process called materialization. Instead of fetching from the database and filtering It would be far more efficient to let SQL Server (which is designed for exactly this kind of operation and may even be able to use indexes if available) do the filtering instead, and transfer a lot less data.
		2. The ‘N+1 Select’ problem: Minimising the trips to the database: Let's say you have a collection of Car objects (database rows), and each Car has a collection of Wheel objects (also rows). In other words, Car -> Wheel is a 1-to-many relationship. Now, let's say you need to iterate through all the cars, and for each one, print out a list of the wheels. The naive O/R implementation would do the following:
			SELECT * FROM Cars;
		And then for each Car:
			SELECT * FROM Wheel WHERE CarId = ?
		In other words, you have one select for the Cars, and then N additional selects, where N is the total number of cars.
		Alternatively, one could get all wheels and perform the lookups in memory:
			SELECT * FROM Wheel
		This reduces the number of round-trips to the database from N+1 to 2. Most ORM tools give you several ways to prevent N+1 selects. One method is to use the Eager Loading data access strategy, which fetches the related data in a single query when you use an Include() statement.
		3. Being too greedy with Columns: Fetching all the columns from a 100 column table when you may only need 2-3 columns is just bad, for obvious reasons.
		4. Mismatched data types: Data types matter, and if not enough attention is paid to them, even disarmingly simple database queries can perform surprisingly poorly. When the data types being compared can vary, The database may resort to a full table scan which can be very time consuming. Generally, these data type mismatches don’t happen if EF creates the database for you and is the only tool to modify its schema. Nevertheless, as soon as someone manually edits either the database or the EF model, the problem can arise. Also, if you build a database externally from EF (such as in SSMS), and then generate an EF model of that database using the Reverse Engineer Code First capability in EF power tools, then it doesn’t apply the column annotation. These days almost all languages use Unicode to represent string objects. To lessen the likelihood of this kind of issue (not to mention other bugs!) I’d always advocate for just using NVARCHAR / NCHAR in the database. You pay a very small extra cost in disk space, but that will probably pay for itself in the very first avoided bug.
		5. Overly-generic queries: When SQL Server runs a query, it uses the values of the provided parameters along with stored statistics about your data to help estimate an efficient execution plan. These statistics include information about the uniqueness and distribution of the data. Because generating the plan has a cost, SQL Server also caches this execution plan so it doesn’t have to be created again – if you run an identical query in the future (even with different parameter values), the plan will be reused. The problem caused by caching the plan for these sorts of generic statements is that Entity Framework will then run an identical query, but with different parameter values. If the query is too generic, a plan which was a good fit for one set of parameter values (when searching against FirstName) may be a poor choice for a different type of search. For example if all pupils live in either New York or Boston, the city column will have very low selectivity and a plan originally generated for pupils with a far more selective LastName may be a poor choice. This problem is called ‘Bad Parameter Sniffing’, and there are far more thorough explanations available elsewhere. It’s worth noting that although these kinds of overly-generic queries make it more likely to hit this kind of issue, it can also occur if a simple query is first run with unrepresentative parameters.
		6. Change tracking: When you retrieve entities from the database, it’s possible that you will modify those objects and expect to be able to write them back to the database. Because Entity Framework doesn’t know your intentions, it has to assume that you will make modifications, so must set these objects up to track any changes you make. That adds extra overhead, and also significantly increases memory requirements. This is particularly problematic when retrieving larger data sets. If you know you only want to read data from a database (for example in an MVC Controller which is just fetching data to pass to a View) you can explicitly tell Entity Framework not to do this tracking. Said another way EF Core supports automatic generation of key values through a variety of mechanisms. When using this feature, a value is generated if the key property is the CLR default--usually zero or null. This means that a graph of entities can be passed to DbContext.Attach or DbSet.Attach and EF Core will mark those entities that have a key already set as Unchanged while those entities that do not have a key set will be marked as Added. This makes it easy to attach a graph of mixed new and existing entities when using generated keys. DbContext.Update and DbSet.Update work in the same way, except that entities with a key set are marked as Modified instead of Unchanged.
		7. Precompiled views: Ordinarily, when EF is first used, it must generate views which are used to work out what queries to run. This work is only done once per app domain, but it can certainly be time consuming. Fortunately there’s no reason this has to be done at runtime – instead you can use precompiled views to save this work. The easiest way to do this is with the Entity Framework Power Tools VS extension. When you have this installed, right click on your context file, then from the Entity Framework menu, choose Generate Views. A new file will be added to your project. Of course there’s a catch: this precompiled view is specific to your context, and if you change anything you’ll need to regenerate the precompiled view – if you don’t, you’ll just get an exception when you try to use EF and nothing will work. But this one is well worth doing, particularly for more complex models.
		8. Giant contexts: Even if you precompile views, Entity Framework still has to do work when a context is first initialized, and that work is proportional to the number of entities in your model. For just a handful of tables it’s not a lot to worry about. However, a common way of working with EF is to automatically generate a context from a pre-existing database, and to simply import all objects. At the time this feels prudent as it maximizes your ability to work with the database. Since even fairly modest databases can contain hundreds of objects, the performance implications quickly get out of control, and startup times can range in the minutes. It’s worth considering whether your context actually needs to know about the entire schema, and if not, to remove those objects.
		9. NGen everything: Most assemblies in the .NET Framework come NGen'd for you automatically – meaning that native code has been pre-JITted. As of Entity Framework 6, the EF assembly isn’t part of this, so it has to be JITted on startup. On slower machines this can take several seconds and will probably take at least a couple of seconds even on a decent machine. Note that you have to separately NGen the 32 and 64 bit versions, and that as well as NGenning EntityFramework.dll it’s also worth NGenning EntityFramework.SqlServer.dll.
		10. DbContext pooling: The basic pattern for using EF Core in an ASP.NET Core application usually involves registering a custom DbContext type into the dependency injection system and later obtaining instances of that type through constructor parameters in controllers. This means a new instance of the DbContext is created for each requests. In version 2.0 we are introducing a new way to register custom DbContext types in dependency injection which transparently introduces a pool of reusable DbContext instances. To use DbContext pooling, use the AddDbContextPool instead of AddDbContext during service registration. If this method is used, at the time a DbContext instance is requested by a controller we will first check if there is an instance available in the pool. Once the request processing finalizes, any state on the instance is reset and the instance is itself returned to the pool. This is conceptually similar to how connection pooling operates in ADO.NET providers and has the advantage of saving some of the cost of initialization of DbContext instance.
		11. Explicitly compiled queries: This is the another opt-in performance features designed to offer benefits in high-scale scenarios. Manual or explicitly compiled query APIs have been available in previous versions of EF and also in LINQ to SQL to allow applications to cache the translation of queries so that they can be computed only once and executed many times. Although in general EF Core can automatically compile and cache queries based on a hashed representation of the query expressions, this mechanism can be used to obtain a small performance gain by bypassing the computation of the hash and the cache lookup, allowing the application to use an already compiled query through the invocation of a delegate.
			// Create an explicitly compiled query
			private static Func<CustomerContext, int, Customer> _customerById =
				EF.CompileQuery((CustomerContext db, int id) =>
					db.Customers
						.Include(c => c.Address)
						.Single(c => c.Id == id));

			// Use the compiled query by invoking it
			using (var db = new CustomerContext())
				{
					var customer = _customerById(db, 147);
			}
		12. Distributed Cache: NCache is an Open Source in-memory distributed cache for .NET. NCache is much faster than database because it’s totally in-memory. And, unlike your database, NCache is linearly scalable because it lets you build a cluster of cache servers and add more servers to the cluster as your transaction loads increase.NCache lets you cache application data so you can reduce those expensive database trips by almost 80%. This reduces load on the database that allows it to perform both reads and writes much faster and not become a performance bottleneck any more.NCache is also an extremely fast and scalable in-memory distributed store for your ASP.NET Core sessions. Additionally, NCache replicates ASP.NET Core sessions to multiple servers to prevent data loss in case any cache server goes down. For ASP.NET Core sessions, this is very important because you cannot afford to lose any sessions at run-time.
		Prior to ASP.NET Core, the older ASP.NET provided a stand-alone ASP.NET Cache that didn’t meet the needs of multi-server environments. Now, ASP.NET Core has introduced IDistributedCache interface as a fairly basic distributed caching standard API that lets you program against it and then plug-in third-party distributed caches seamlessly. NCache has also implemented a provider for IDistributedCache that you can plug into your ASP.NET Core applications. This way, you don’t have to change any code specific to NCache.
		13. ASP.net Core sessions in Distributed cache: Prior to ASP.NET Core, the older ASP.NET provided an ASP.NET Session State Provider framework that allowed third-party session storage providers to plug-in. ASP.NET Core sessions provides a similar mechanism for plugging in third-party storage providers.There are two ways to use NCache as ASP.NET Core session storage. They are:
			1. Use NCache for ASP.NET Core Sessions thru IDistributedCache
				As soon as you configure NCache as IDistributedCache provider for ASP.NET Core, NCache automatically becomes the default storage option for ASP.NET Core sessions and you don’t have to do anything else. But please note that this implementation is limited in features as compared to the older (before ASP.NET Core) ASP.NET Session State.

			Here are some of the things the default ASP.NET Core Sessions implementation lacks:

				a. No session locking is provided in ASP.NET Core. This is something even older ASP.NET Session State had provided.
				B. byte[] array for custom objects: ASP.NET Core forces you to convert all your custom objects into byte array before you can store is in the session. Even older ASP.NET Session State support custom objects.
			2. Use NCache as ASP.NET Core Sessions Provider
				To work around the default ASP.NET Core Sessions implementation through IDistributedCacheprovider, NCache has implemented its own ASP.NET Core Sessions provider. This implementation has a lot more features than the default one.

		Microsoft provides two options as IDistributedCache providers. One is SQL Server and second is Redis. NCache is better than both options. In comparison to SQL Server, NCache is much faster and more scalable.
		NCache is also a better than Redis for the following reasons

			Native .NET: NCache is 100% native .NET and therefore fits into your .NET application stack very nicely. On the other hand, Redis comes from a Linux background and is not a native .NET cache.
			Faster than Redis: NCache is actually faster than Redis. NCache Client Cache feature gives NCache a significant performance boost.
			More Features: NCache offers a number of very important distributed cache features that Redis does not. See more details in this Redis vs NCache.

	.Net Core Performance Tools:
		1. Pre-JIT .NET Core Applications with CrossGen: There's a tool called crossgen.exe that comes with the .NET Core framework that you can use to pre-compile your .NET Core application to avoid the initial JIT compile step. This is not the compile to native that was briefly promised for .NET Core, but merely a step that promises to improve the first load experience.
		2. Make your .NET Core Application Smaller with Mono’s Linker: The .NET Core team has been working on a linker in collaboration with the Mono team. It is based on the Mono Linker project, and by extension, on the Cecil project. The goal of the .NET IL Linker project is two part:
			Provide a capable linker for .NET Core to make it easy to significantly reduce .NET Core application sizes.
			Establish the Mono Linker as the primary linker for the .NET ecosystem.
		In trivial cases, the linker can reduce the size of applications by 50%. The size wins may be more favorable or more moderate for larger applications. The linker removes code in your application and dependent libraries that are not reached by any code paths. It is effectively an application-specific dead code analysis.
		3. Faster Startup with ASP.NET Core Precompiled Views: Add the Microsoft.AspNetCore.Mvc.Razor.ViewCompilation package to your project. You can do this through the dotnet cli dotnet add package Microsoft.AspNetCore.Mvc.Razor.ViewCompilation or through the visual studio package explorer install-package Microsoft.AspNetCore.Mvc.Razor.ViewCompilation. Then simply add <MvcRazorCompileOnPublish>true</MvcRazorCompileOnPublish> and <PreserveCompilationContext>true</PreserveCompilationContext> to your csproj file’s property group. Thats it. You will now precompile your razor views on publish.

	Performance monitoring in rpoduction
		1. Monitoring .NET Core Applications with Dynatrace: Dynatrace provides out-of-the-box support for ASP.NET Core (versions 1.0 and 1.1). For each web request, you get response times, CPU usage, and locking times. Dynatrace also detects each failed request and provides an exception stacktrace that shows you the root cause of the failure. Dynatrace monitors every single transaction, so you won’t miss any errors. OneAgent monitors all ADO.NET-based database interactions, including Entity Framework Core. You’ll see execution times of every single SQL statement. Dynatrace monitors the CPU usage of threads as they process each transaction. Additionally, OneAgent monitors background activity threads (i.e, CPU time that’s spent on threads that handle background tasks). If you’re not running a web application, but rather a standalone executable (GUI application, batch process, etc), you can specify any method in your code to act as the executable’s entrypoint. If you’re using IIS, Apache, or Nginx as a web server for your ASP.NET Core application, you can take advantage of real user monitoring. This means that you can track visits, user actions, browser rendering performance, and JavaScript errors.
		2. Tracking Custom Dependencies with Application Insights: A dependency is an external component that is called by your app. It's typically a service called using HTTP, or a database, or a file system. Application Insights measures how long your application waits for dependencies and how often a dependency call fails. You can investigate specific calls, and relate them to requests and exceptions. Partial dependency information is collected automatically by the Application Insights SDK. To get complete data, install the appropriate agent for the host server.
		3. Tracking Slow Requests and Performance Testing with Application Insights: Application Insights is an extensible analytics platform that monitors the performance and usage of your live ASP.NET Core web applications. With Asp.Net Core 2.0, ApplicationInsights is included by default when running from Visual Studio 2017. You only need to configure the instrumentation key so that telemetry is sent to Application Insights service. If instrumentation key is not added: If running from outside Visual Studio 2017, like from a command-line (using dotnet run myapp.dll) or other IDE like Visual Studio Code, then Application Insights need to be explicitly added/configured.









This also means calling local functions is cheaper and they can be inlined, possibly increasing performance even further.

		The performance improvements in C# 7.0 focus on reducing the copying of data between memory locations.

Local functions allow declaration of helper functions nested inside other functions. This does not only reduce their scope, but also allow the use of variables declared in their encompassing scope, without allocating additional memory on heap or stack:

static void ReduceMagicalEffects(this IWeapon weapon, int timePassed)
{
    double decayRate = CalculateDecayRate();
    double GetRemainingEffect(double currentEffect) =>
        currentEffect * Math.Pow(decayRate, timePassed);

    weapon.FireEffect = GetRemainingEffect(weapon.FireEffect);
    weapon.IceEffect = GetRemainingEffect(weapon.IceEffect);
    weapon.LightningEffect = GetRemainingEffect(weapon.LightningEffect);
}
Return values and local variables by reference can also be used to prevent unnecessary copying of data. At the same time, they modify the behavior. Since the variable is pointing at the original memory location, any changes to the value there will of course also affect the local variable value:

[Test]
public void LocalVariableByReference()
{
    var terrain = Terrain.Get();

    ref TerrainType terrainType = ref terrain.GetAt(4, 2);
    Assert.AreEqual(TerrainType.Grass, terrainType);

    // modify enum value at the original location
    terrain.BurnAt(4, 2);
    // local value was also affected
    Assert.AreEqual(TerrainType.Dirt, terrainType);
}
In the above example, terrainType is a local variable by reference, and GetAt is a function returning a value by reference:

public ref TerrainType GetAt(int x, int y) => ref terrain[x, y];
The final performance improvement in C# 7.0 targets asynchronous methods.

Currently all asynchronous methods must return Task<T>, which is a reference type. Now, a value type is available as well: ValueTask.

If the asynchronous method already has the result available when called, this can lead to significant performance improvements due to reduced number of allocations on the heap.

Additionally, the language now allows other custom return types for asynchronous methods, which might make sense for libraries with specific requirements.
